<!DOCTYPE html><html domain=thefortunedays.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href="/img/favicon/favicon-192x192.png?hash=735479e2c4" rel=icon type=image/png><meta content=#ff577d name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>Getting Started With Apache Spark</title><meta content="Getting Started With Apache Spark" property=og:title><meta content="Getting started with Apache Spark and Structure Streaming on Kubernetes." name=description><meta content="Getting started with Apache Spark and Structure Streaming on Kubernetes." property=og:description><meta content=summary_large_image name=twitter:card><meta content=@phamthethanh108 name=twitter:site><meta content=@phamthethanh108 name=twitter:creator><meta content=https://thefortunedays.com/img/remote/Z1o6V9l.jpg property=og:image><link href=https://thefortunedays.com/articles/getting-started-with-apache-spark/ rel=canonical><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="The Fortune Days"><link href=/ rel=preconnect crossorigin=""><link href=/fonts/CyberwayRiders.woff2 rel=preload crossorigin="" as=font type=font/woff2><link href=https://fonts.googleapis.com rel=preconnect><link href=https://fonts.gstatic.com rel=preconnect crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel=stylesheet><script async src="/js/min.js?hash=8db32dcb3a" data-cwv-src="/js/web-vitals.js?hash=4a3c7b61e1" defer></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-9X9G2YCR2X"></script><script>window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9X9G2YCR2X');</script><script csp-hash="">if (/Mac OS X/.test(navigator.userAgent))document.documentElement.classList.add('apple')</script><style>:root{--primary: #ff577d;--primary-dark:#ff184c;--text-color: #F5F5F7;--bg-color: 	#003062;--nav-bg-color: #ff577d;--nav-text-color: #fff;--footer-bg-color: #091833;--footer-text-color: #999;--main-width: calc(100vw - 3em)}main img{content-visibility:auto}article *{scroll-margin-top:50px}header nav{position:fixed;padding:.575em 1.5em;background:var(--nav-bg-color);color:var(--nav-text-color);font-weight:200;text-align:right}@media (min-width:37.5em){:root{--main-width: calc(37.5em - 3em)}}dialog,share-widget{position:fixed;opacity:.9}share-widget{right:20px;bottom:20px}share-widget div{width:10px;height:10px;background-image:url(/img/share.svg);background-repeat:no-repeat;background-position:center}.apple share-widget div{background-image:url(/img/share-apple.svg)}body,share-widget button{margin:0}share-widget button:active{transform:scale(1.2)}dialog{background-color:#0a9cf5;z-index:1000}#nav{z-index:2;position:relative}#reading-progress,header nav{z-index:1;width:100vw;left:0;top:0}#reading-progress{background-color:var(--primary);position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}@font-face{font-display:optional;font-family:"CyberwayRiders";font-weight:100 900;font-style:oblique 0deg 10deg;src:url(/fonts/CyberwayRiders.woff2) format("woff2")}button,html{line-height:1.15}html{-webkit-text-size-adjust:100%;font-size:17.5px;font-family:"Montserrat",sans-serif;--font-family: "Montserrat", sans-serif}pre{font-size:1em}a{background-color:transparent;color:#f9c412;text-decoration:none;color:var(--primary)}b{font-weight:700}img{border-style:none;max-width:100%;height:auto;margin:0 auto}button{font-family:inherit;font-size:100%;overflow:visible;text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}summary{display:list-item}h1,h2,h3{margin:3rem 0 1.38rem;line-height:1.3}h1{font-size:2.488rem}h2{font-size:2.074rem}h3{font-size:1.728rem}body,p,pre,ul{font-size:1rem;line-height:1.6}p,pre,ul{margin-bottom:1.36rem}code,pre{overflow-x:auto}pre{font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace}pre:has(code:not([class])){background:#2d2d2d}pre code:not([class]){color:#ccc;padding:0;overflow-x:scroll}button,code{border-radius:.3em}code{color:#e2777a;padding:0 .3em;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:90%;background:#2d2d2d}h1,h2,h3{font-family:var(--font-family)}h2{font-style:italic}a:hover{text-decoration:underline}button{max-width:100%;background:#f2f2f2;color:#191919;cursor:pointer;display:inline-block;padding:.75em 1.5em;text-align:center;margin:0 .75em 1.5em 0}button:hover{background:#d9d9d9;color:#000}button:not([disabled]),button[type=submit]{background:#f9c412;color:#181818}button:not([disabled]):hover,button[type=submit]:hover{background:#ba9005;color:#000}*{border:0;box-sizing:border-box}body,header nav a{font-family:var(--font-family)}body{background:var(--bg-color);color:var(--text-color)}header{padding:4em 1.5em 3em;width:37.5em;margin:0 auto;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header p,ul{margin-top:0}header nav h1{float:left;font-size:inherit;line-height:inherit;margin:0;text-align:left}header nav a{font-weight:700;text-decoration:none;color:var(--nav-text-color);margin-left:1.5em;font-family:'CyberwayRiders';text-shadow:0 0 7px #fff,0 0 42px #0a9cf5}header nav a:first-of-type{margin-left:auto}header nav a:last-of-type{margin-right:1.5em}main{max-width:70rem;margin:0 auto}footer{background:var(--footer-bg-color);color:var(--footer-text-color);padding:3em;text-align:center}footer>*{margin:1.5em;font-family:'CyberwayRiders';text-shadow:0 0 7px #fff,0 0 42px #0a9cf5}footer nav a img{vertical-align:middle}footer nav,footer p{font-size:90%}article{max-width:100%;padding:1.5em;width:37.5em;margin:0 auto;min-height:100vh}li ul{margin-bottom:0}code[class*=language-],pre[class*=language-]{color:#ccc;background:0 0;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-]{padding:0}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background:#2d2d2d}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.token.comment{color:#999}.token.punctuation{color:#ccc}.token.namespace,.token.tag{color:#e2777a}.token.function,.token.number{color:#f08d49}.token.class-name,.token.constant,.token.property{color:#f8c555}.token.atrule,.token.builtin,.token.keyword{color:#cc99cd}.token.string,.token.variable{color:#7ec699}.token.operator,.token.url{color:#67cdcc}.token.inserted{color:green}button:not([disabled]),button[type=submit]{background:var(--primary)}button:not([disabled]):hover,button[type=submit]:hover{background:var(--primary-dark)}.about,.tags{min-height:80vh}.about h1{font-family:'CyberwayRiders';padding-bottom:1em}.home h1{padding-bottom:1em;color:#fff}.home h1,article h1{text-shadow:0 0 7px #fff,0 0 42px #0a9cf5}.home h1,.tags h1,article h1,h2,h3{font-family:'CyberwayRiders'}.tags h1,article h1{padding-bottom:1em;text-align:center}h2,h3{text-shadow:0 0 7px #fff,0 0 42px #0a9cf5}@media (min-width:600px){.about,.tags{min-height:calc(100vh - 360px)}}</style><header><nav><div id=nav><h1><a href=/ title=Homepage>The Fortune Days</a></h1><a href=/tags/ >Tags</a> <a href=/about/ >About</a></div><div id=reading-progress aria-hidden=true></div></nav><dialog id=message></dialog></header><main><article><h1>Getting Started With Apache Spark</h1><p>This blog post is a summary of my presentation on Apache Spark Overview with a basic Kafka streaming application. It includes some basic steps to create and deploy a simple Spark application. This is not a comprehensive tutorial, so for more information, please visit the Apache Spark <a href=https://spark.apache.org/docs/latest/ >website</a>.<h2 id=prerequisite>Prerequisite <a href=#prerequisite class=direct-link>#</a></h2><ul><li><a href=https://www.docker.com/ >Docker</a><li><a href=https://minikube.sigs.k8s.io/docs/ >Minikube</a></ul><h2 id=apache-spark-overview>Apache Spark Overview <a href=#apache-spark-overview class=direct-link>#</a></h2><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.avif 1920w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1280w.avif 1280w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-640w.avif 640w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-320w.avif 320w" type=image/avif><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.webp 1920w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1280w.webp 1280w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-640w.webp 640w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-320w.webp 320w" type=image/webp><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.jpg 1920w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1280w.jpg 1280w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-640w.jpg 640w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-320w.jpg 320w" type=image/jpeg><img alt="Introduction to Apache Spark" decoding=async height=3186 loading=lazy src=/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.jpg style="background-size:cover;background-image:url(&#34;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 893 3186'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAPCAIAAABMVPnqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAjElEQVQImSWOyw7DMAjA+P8v3HnPVF1pgEAIVcsU7WzLMmRmJW6imQnWu/XRPa5McGuttWP4UAVrRMyVaLjDtm211qWUiACh2kTkT0R1R2TmWfPh5n67ly8pxHGQ+fJ4ju6QmR7BLFM7IsyM6t67gakQMa5vxgKIyNJwfdH2ATWdBzuaClx5qs/bvM4fnKmr3FK+eDQAAAAASUVORK5CYII='%3E%3C/image%3E%3C/svg%3E&#34;)" width=893></picture><h2 id=build-spark-application>Build Spark Application <a href=#build-spark-application class=direct-link>#</a></h2><p><code>Main.scala</code><pre class=language-scala><code class=language-scala><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span></span>SparkSession<br><br><br><br><span class="token keyword">object</span> Main <span class="token punctuation">{</span><br>  <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span><br>    <span class="token keyword">val</span> spark<span class="token operator">:</span>SparkSession <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">"hello-spark"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">import</span> <span class="token namespace">spark<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span></span>_<br>    <span class="token keyword">val</span> kafkaOpts <span class="token operator">=</span> Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><br>      <span class="token string">"kafka.bootstrap.servers"</span><span class="token operator">-></span> <span class="token string">"kafka:9092"</span><span class="token punctuation">,</span><br>      <span class="token string">"subscribe"</span><span class="token operator">-></span> <span class="token string">"test"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.sasl.mechanism"</span><span class="token operator">-></span> <span class="token string">"PLAIN"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.security.protocol"</span> <span class="token operator">-></span> <span class="token string">"SASL_PLAINTEXT"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.sasl.jaas.config"</span><span class="token operator">-></span> <span class="token string triple-quoted-string">"""org.apache.kafka.common.security.plain.PlainLoginModule required username="user1" password="kk3gaqZRly";"""</span><span class="token punctuation">,</span><br>    <span class="token punctuation">)</span><br>    <span class="token keyword">val</span> df <span class="token operator">=</span> spark<br>      <span class="token punctuation">.</span>readStream<br>      <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"kafka"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>options<span class="token punctuation">(</span>kafkaOpts<span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">val</span> ds <span class="token operator">=</span> df<span class="token punctuation">.</span>selectExpr<span class="token punctuation">(</span><span class="token string">"CAST(key AS STRING)"</span><span class="token punctuation">,</span> <span class="token string">"CAST(value AS STRING)"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>as<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token punctuation">]</span><br><br>    ds<span class="token punctuation">.</span>writeStream<br>      <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"console"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>outputMode<span class="token punctuation">(</span><span class="token string">"append"</span><span class="token punctuation">)</span><br>      <span class="token comment">//.trigger(Trigger.Continuous(1))</span><br>      <span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span><br>  <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre><p>Note that in the above code we put the user/password of Kafka directly in the code which is just for simplicity & testing purpose. For production please store them in Vault or Secret instead.<p><code>build.sbt</code><pre class=language-shell><code class=language-shell>ThisBuild / version :<span class="token operator">=</span> <span class="token string">"0.1.0-SNAPSHOT"</span><br><br>ThisBuild / scalaVersion :<span class="token operator">=</span> <span class="token string">"2.12.18"</span><br><br>libraryDependencies <span class="token operator">+=</span> <span class="token string">"org.apache.spark"</span> %% <span class="token string">"spark-core"</span> % <span class="token string">"3.5.1"</span><br>libraryDependencies <span class="token operator">+=</span> <span class="token string">"org.apache.spark"</span> %% <span class="token string">"spark-sql"</span> % <span class="token string">"3.5.1"</span><br>libraryDependencies <span class="token operator">+=</span> <span class="token string">"org.apache.spark"</span> %% <span class="token string">"spark-sql-kafka-0-10"</span> % <span class="token string">"3.5.1"</span> % Test<br><br>Compile / run / mainClass :<span class="token operator">=</span> Some<span class="token punctuation">(</span><span class="token string">"Main"</span><span class="token punctuation">)</span><br><br>lazy val root <span class="token operator">=</span> <span class="token punctuation">(</span>project <span class="token keyword">in</span> file<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">))</span><br>  .settings<span class="token punctuation">(</span><br>    name :<span class="token operator">=</span> <span class="token string">"hello-spark"</span><br>  <span class="token punctuation">)</span><br></code></pre><p><code>Dockerfile</code><pre class=language-dockerfile><code class=language-dockerfile><span class="token instruction"><span class="token keyword">ARG</span> SCALA_VERSION=2.12</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_JAR_NAME=hello-spark_2.12-0.1.0-SNAPSHOT.jar</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_MAIN_CLASS=Main</span><br><br><span class="token instruction"><span class="token keyword">FROM</span> sbtscala/scala-sbt:graalvm-ce-22.3.3-b1-java17_1.9.9_2.12.18 <span class="token keyword">as</span> build</span><br><span class="token instruction"><span class="token keyword">WORKDIR</span> /app</span><br><span class="token instruction"><span class="token keyword">COPY</span> . .</span><br><span class="token instruction"><span class="token keyword">RUN</span> sbt package</span><br><br><span class="token instruction"><span class="token keyword">FROM</span> apache/spark:3.5.1-scala2.12-java17-ubuntu</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_JAR_NAME</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_MAIN_CLASS</span><br><span class="token instruction"><span class="token keyword">ARG</span> SCALA_VERSION</span><br><span class="token instruction"><span class="token keyword">COPY</span> <span class="token options"><span class="token property">--from</span><span class="token punctuation">=</span><span class="token string">build</span></span> /app/target/scala-<span class="token variable">${SCALA_VERSION}</span>/<span class="token variable">${ARG_JAR_NAME}</span> /app/work/application.jar</span></code></pre><p>Build application Docker image<pre class=language-shell><code class=language-shell><span class="token builtin class-name">eval</span> <span class="token variable"><span class="token variable">$(</span>minikube -p minikube docker-env<span class="token variable">)</span></span><br><span class="token function">docker</span> build -t hello-spark:latest <span class="token builtin class-name">.</span></code></pre><h2 id=deploy-kafka>Deploy Kafka <a href=#deploy-kafka class=direct-link>#</a></h2><pre class=language-shell><code class=language-shell>helm <span class="token function">install</span> kafka oci://registry-1.docker.io/bitnamicharts/kafka</code></pre><p>Notice the notes after you installed Kafka, it includes information for authenticate with Kafka cluster. Otherwise you can check the <code>server.properties</code>:<pre class=language-shell><code class=language-shell>kubectl <span class="token builtin class-name">exec</span> kafka-controller-0 -- <span class="token function">cat</span> /opt/bitnami/kafka/config/server.properties</code></pre><p>Let run Kafka-client to create topic and test our Kafka cluster.<pre class=language-shell><code class=language-shell>kubectl run kafka-client --rm -ti --image bitnami/kafka:3.6.1-debian-12-r12 -- <span class="token function">bash</span><br><span class="token builtin class-name">cd</span> /opt/bitnami/kafka/bin<br><br><span class="token function">cat</span> <span class="token operator">>></span> client.conf <span class="token operator">&lt;&lt;</span><span class="token string">EOF<br>security.protocol=SASL_PLAINTEXT<br>sasl.mechanism=PLAIN<br>sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="user1" password="kk3gaqZRly";<br>EOF</span><br><br>kafka-topics.sh --create --topic <span class="token builtin class-name">test</span> --bootstrap-server kafka:9092 --command-config client.conf<br><br>kafka-console-producer.sh --topic <span class="token builtin class-name">test</span> --request-required-acks all --bootstrap-server kafka:9092 --producer.config client.conf<br><br>kafka-console-consumer.sh --topic <span class="token builtin class-name">test</span> --bootstrap-server kafka:9092 --consumer.config client.conf<br></code></pre><h2 id=deployment>Deployment <a href=#deployment class=direct-link>#</a></h2><p>Deploy Spark application to Kubernetes cluster require a ServiceAccount, hence we need to create it first:<pre class=language-shell><code class=language-shell>kubectl create serviceaccount sparksubmit<br>kubectl create clusterrolebinding sparksubmit-role --clusterrole<span class="token operator">=</span>edit --serviceaccount<span class="token operator">=</span>default:sparksubmit --namespace<span class="token operator">=</span>default</code></pre><p>Deploy a Spark application to Kubernetes can be done by 2 ways:<ul><li>Using spark-submit<li>Using Spark-operator</ul><h3 id=deploy-using-spark-submit>Deploy using spark-submit <a href=#deploy-using-spark-submit class=direct-link>#</a></h3><p>In this deployment mode, we submit the application using <code>spark-submit</code> from outside of the Kubernetes cluster. To install the spark-submit, you can download and setup the Apache Spark on your machine or pipeline:<pre class=language-shell><code class=language-shell><span class="token function">wget</span> https://www.apache.org/dyn/closer.lua/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz<br><span class="token function">tar</span> -xzf spark-3.5.1-bin-hadoop3.tgz -C ~/bin<br><br><span class="token builtin class-name">export</span> <span class="token variable assign-left"><span class="token constant environment">PATH</span></span><span class="token operator">=</span><span class="token constant environment">$PATH</span>:~/bin/spark-3.5.1-bin-hadoop3<br><span class="token builtin class-name">export</span> <span class="token variable assign-left"><span class="token constant environment">PATH</span></span><span class="token operator">=</span><span class="token constant environment">$PATH</span>:~/bin/spark-3.5.1-bin-hadoop3/bin<br><span class="token builtin class-name">export</span> <span class="token variable assign-left">SPARK_HOME</span><span class="token operator">=~</span>/bin/spark-3.5.1-bin-hadoop3<br><span class="token builtin class-name">export</span> <span class="token variable assign-left">HADOOP_HOME</span><span class="token operator">=~</span>/bin/spark-3.5.1-bin-hadoop3</code></pre><p>Note that for testing the Spark application locally, you also must install Java (17) and Scala (2.12) on your local machine.<p>After installed <code>spark-submit</code>, go ahead to submit the Spark application:<pre class=language-shell><code class=language-shell>spark-submit <span class="token punctuation">\</span><br>--master k8s://https://127.0.0.1:32769 <span class="token punctuation">\</span><br>--deploy-mode cluster <span class="token punctuation">\</span><br>--name hello-spark <span class="token punctuation">\</span><br>--class Main <span class="token punctuation">\</span><br>--conf spark.kubernetes.namespace<span class="token operator">=</span>default <span class="token punctuation">\</span><br>--conf spark.kubernetes.container.image<span class="token operator">=</span>hello-spark:latest <span class="token punctuation">\</span><br>--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 <span class="token punctuation">\</span><br>--conf spark.driver.extraJavaOptions<span class="token operator">=</span><span class="token string">"-Divy.cache.dir=/tmp -Divy.home=/tmp"</span> <span class="token punctuation">\</span><br>--conf spark.kubernetes.authenticate.driver.serviceAccountName<span class="token operator">=</span>sparksubmit <span class="token punctuation">\</span><br>--conf spark.kubernetes.authenticate.submission.caCertFile<span class="token operator">=~</span>/.minikube/ca.crt <span class="token punctuation">\</span><br>local:///app/work/application.jar</code></pre><p>Notes:<ul><li>Run <code>kubectl cluster-info</code> to get the URL of your K8S cluster.<li>Replace the <code>caCertFile</code> to your certificate file. If relative path doesn't work, change to absolute path.</ul><p>After deployed, you would see the driver and executors got deployed in your K8S:<pre class=language-shell><code class=language-shell>kubectl get pod<br>NAME                                  READY   STATUS    RESTARTS      AGE<br>hello-spark-3605288e2b93f981-exec-1   <span class="token number">1</span>/1     Running   <span class="token number">0</span>             10s<br>hello-spark-3605288e2b93f981-exec-2   <span class="token number">1</span>/1     Running   <span class="token number">0</span>             10s<br>hello-spark-6da76b8e2b92dd56-driver   <span class="token number">1</span>/1     Running   <span class="token number">0</span>             82s<br>kafka-controller-0                    <span class="token number">1</span>/1     Running   <span class="token number">1</span> <span class="token punctuation">(</span>12m ago<span class="token punctuation">)</span>   14h<br>kafka-controller-1                    <span class="token number">1</span>/1     Running   <span class="token number">1</span> <span class="token punctuation">(</span>12m ago<span class="token punctuation">)</span>   14h<br>kafka-controller-2                    <span class="token number">1</span>/1     Running   <span class="token number">1</span> <span class="token punctuation">(</span>12m ago<span class="token punctuation">)</span>   14h</code></pre><p>Once the driver and the executors are up and running, you can start the Spark UI to see the application detail as well as start sending some events to Kafka and check for the output from log of the driver.<pre class=language-shell><code class=language-shell>kubectl port-forward hello-spark-6da76b8e2b92dd56-driver <span class="token number">4040</span>:4040</code></pre><p>If you would like to kill the application, just run spark-submit with <code>-kill</code> option:<pre class=language-shell><code class=language-shell>spark-submit --kill hello-spark-d5358c8e1cd2e8b0-driver --master k8s://https://127.0.0.1:32769</code></pre><h3 id=deploy-using-spark-operator>Deploy using Spark-Operator <a href=#deploy-using-spark-operator class=direct-link>#</a></h3><p>Deploy Spark application using Spark operator is recommended for production. In this deployment model, we will install <code>spark-operator</code> inside K8S cluster, spark-operator will listen on any application deployed to the K8S with application kind as <code>SparkApplication</code> and will help us to submit Spark application to the K8S for us.<p>You can read more detail on using spark-operator at their <a href=https://github.com/kubeflow/spark-operator/blob/master/docs/quick-start-guide.md>user-guide</a><p>Install spark-operator:<pre class=language-shell><code class=language-shell>helm repo <span class="token function">add</span> spark-operator https://kubeflow.github.io/spark-operator<br>helm <span class="token function">install</span> spark-operator spark-operator/spark-operator --namespace spark-operator --create-namespace --set <span class="token variable assign-left">enableWebhook</span><span class="token operator">=</span>true</code></pre><p>Note that the option <code>--set enableWebhook=true</code> must be enabled if you would like to use some certain Spark operator features such as setting <code>env</code> or GPU for driver and executor. See more <a href=https://github.com/kubeflow/spark-operator/blob/master/docs/quick-start-guide.md#mutating-admission-webhooks-on-a-private-gke-or-eks-cluster>here</a><p>Check for the deployment status:<pre class=language-shell><code class=language-shell>kubectl get pod -n spark-operator<br>NAME                              READY   STATUS    RESTARTS      AGE<br>spark-operator-675d97df85-c2b9g   <span class="token number">1</span>/1     Running   <span class="token number">2</span> <span class="token punctuation">(</span>46m ago<span class="token punctuation">)</span>   14h</code></pre><p>Create application deployment file <code>spark-application.yaml</code><pre class=language-yaml><code class=language-yaml><span class="token atrule key">apiVersion</span><span class="token punctuation">:</span> sparkoperator.k8s.io/v1beta2<br><span class="token atrule key">kind</span><span class="token punctuation">:</span> SparkApplication<br><span class="token atrule key">metadata</span><span class="token punctuation">:</span><br>  <span class="token atrule key">name</span><span class="token punctuation">:</span> hello<span class="token punctuation">-</span>spark<br>  <span class="token atrule key">namespace</span><span class="token punctuation">:</span> default<br><span class="token atrule key">spec</span><span class="token punctuation">:</span><br>  <span class="token atrule key">type</span><span class="token punctuation">:</span> Scala<br>  <span class="token atrule key">sparkVersion</span><span class="token punctuation">:</span> 3.5.1<br>  <span class="token atrule key">mode</span><span class="token punctuation">:</span> cluster<br>  <span class="token atrule key">image</span><span class="token punctuation">:</span> hello<span class="token punctuation">-</span>spark<span class="token punctuation">:</span>latest<br>  <span class="token atrule key">mainClass</span><span class="token punctuation">:</span> Main<br>  <span class="token atrule key">mainApplicationFile</span><span class="token punctuation">:</span> local<span class="token punctuation">:</span>///app/work/application.jar<br>  <span class="token atrule key">deps</span><span class="token punctuation">:</span><br>    <span class="token atrule key">packages</span><span class="token punctuation">:</span><br>      <span class="token punctuation">-</span> org.apache.spark<span class="token punctuation">:</span>spark<span class="token punctuation">-</span>sql<span class="token punctuation">-</span>kafka<span class="token punctuation">-</span>0<span class="token punctuation">-</span>10_2.12<span class="token punctuation">:</span>3.5.1<br>  <span class="token atrule key">sparkConf</span><span class="token punctuation">:</span><br>    <span class="token atrule key">"spark.driver.extraJavaOptions"</span><span class="token punctuation">:</span> <span class="token string">"-Divy.cache.dir=/tmp -Divy.home=/tmp"</span><br>  <span class="token atrule key">driver</span><span class="token punctuation">:</span><br>    <span class="token atrule key">memory</span><span class="token punctuation">:</span> 512m<br>    <span class="token atrule key">labels</span><span class="token punctuation">:</span><br>      <span class="token atrule key">version</span><span class="token punctuation">:</span> 3.5.1<br>    <span class="token atrule key">serviceAccount</span><span class="token punctuation">:</span> sparksubmit<br>  <span class="token atrule key">executor</span><span class="token punctuation">:</span><br>    <span class="token atrule key">memory</span><span class="token punctuation">:</span> 512m<br>    <span class="token atrule key">instances</span><span class="token punctuation">:</span> <span class="token number">3</span><br>    <span class="token atrule key">labels</span><span class="token punctuation">:</span><br>      <span class="token atrule key">version</span><span class="token punctuation">:</span> 3.5.1</code></pre><p>Deploy it to the cluster<pre class=language-shell><code class=language-shell>kubectl apply -f spark-application.yaml</code></pre><p>Wait for sometime for spark-operator to detect and submit the application and then you can start testing the application:<pre class=language-shell><code class=language-shell>kubectl get pod<br>NAME                                  READY   STATUS    RESTARTS      AGE<br>hello-spark-driver                    <span class="token number">1</span>/1     Running   <span class="token number">0</span>             54s<br>hello-spark-efd4f18e2bb34946-exec-1   <span class="token number">1</span>/1     Running   <span class="token number">0</span>             4s<br>hello-spark-efd4f18e2bb34946-exec-2   <span class="token number">1</span>/1     Running   <span class="token number">0</span>             4s<br>hello-spark-efd4f18e2bb34946-exec-3   <span class="token number">1</span>/1     Running   <span class="token number">0</span>             4s<br>kafka-controller-0                    <span class="token number">1</span>/1     Running   <span class="token number">1</span> <span class="token punctuation">(</span>46m ago<span class="token punctuation">)</span>   14h<br>kafka-controller-1                    <span class="token number">1</span>/1     Running   <span class="token number">1</span> <span class="token punctuation">(</span>46m ago<span class="token punctuation">)</span>   14h<br>kafka-controller-2                    <span class="token number">1</span>/1     Running   <span class="token number">1</span> <span class="token punctuation">(</span>46m ago<span class="token punctuation">)</span>   14h</code></pre><p>At this step, we've done for the very basic Spark application from development to deployment. But as mentioned earlier, we should not put username and password directly in the code but should store them in Vault or Secret. By applying spark-operator, we can easily do this using <code>env</code> config of the operator(but remember to enable option <code>enableWebhook=true</code> when installing spark-operator)<p>Let update the code and the deployment file as below:<p><code>Main.scala</code><pre class=language-scala><code class=language-scala><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span></span>SparkSession<br><br><br><br><span class="token keyword">object</span> Main <span class="token punctuation">{</span><br>  <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span><br>    <span class="token keyword">val</span> spark<span class="token operator">:</span>SparkSession <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">"hello-spark"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">import</span> <span class="token namespace">spark<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span></span>_<br>    <span class="token keyword">val</span> user <span class="token operator">=</span> sys<span class="token punctuation">.</span>env<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span><span class="token string">"KAFKA_USER"</span><span class="token punctuation">,</span> <span class="token string">"user1"</span><span class="token punctuation">)</span><br>    <span class="token keyword">val</span> pass <span class="token operator">=</span> sys<span class="token punctuation">.</span>env<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span><span class="token string">"KAFKA_PASS"</span><span class="token punctuation">,</span> <span class="token string">"xxx"</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">val</span> kafkaOpts <span class="token operator">=</span> Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><br>      <span class="token string">"kafka.bootstrap.servers"</span><span class="token operator">-></span> <span class="token string">"kafka:9092"</span><span class="token punctuation">,</span><br>      <span class="token string">"subscribe"</span><span class="token operator">-></span> <span class="token string">"test"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.sasl.mechanism"</span><span class="token operator">-></span> <span class="token string">"PLAIN"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.security.protocol"</span> <span class="token operator">-></span> <span class="token string">"SASL_PLAINTEXT"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.sasl.jaas.config"</span><span class="token operator">-></span> <span class="token string-interpolation"><span class="token function id">s</span><span class="token string">"""org.apache.kafka.common.security.plain.PlainLoginModule required username="</span><span class="token interpolation"><span class="token punctuation">$</span><span class="token expression">user</span></span><span class="token string">" password="</span><span class="token interpolation"><span class="token punctuation">$</span><span class="token expression">pass</span></span><span class="token string">";"""</span></span><span class="token punctuation">,</span><br>    <span class="token punctuation">)</span><br>    <span class="token keyword">val</span> df <span class="token operator">=</span> spark<br>      <span class="token punctuation">.</span>readStream<br>      <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"kafka"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>options<span class="token punctuation">(</span>kafkaOpts<span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">val</span> ds <span class="token operator">=</span> df<span class="token punctuation">.</span>selectExpr<span class="token punctuation">(</span><span class="token string">"CAST(key AS STRING)"</span><span class="token punctuation">,</span> <span class="token string">"CAST(value AS STRING)"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>as<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token punctuation">]</span><br><br>    ds<span class="token punctuation">.</span>writeStream<br>      <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"console"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>outputMode<span class="token punctuation">(</span><span class="token string">"append"</span><span class="token punctuation">)</span><br>      <span class="token comment">//.trigger(Trigger.Continuous(1))</span><br>      <span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span><br>  <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre><p><code>spark-application.yaml</code><pre class=language-yaml><code class=language-yaml><span class="token atrule key">apiVersion</span><span class="token punctuation">:</span> sparkoperator.k8s.io/v1beta2<br><span class="token atrule key">kind</span><span class="token punctuation">:</span> SparkApplication<br><span class="token atrule key">metadata</span><span class="token punctuation">:</span><br>  <span class="token atrule key">name</span><span class="token punctuation">:</span> hello<span class="token punctuation">-</span>spark<br>  <span class="token atrule key">namespace</span><span class="token punctuation">:</span> default<br><span class="token atrule key">spec</span><span class="token punctuation">:</span><br>  <span class="token atrule key">type</span><span class="token punctuation">:</span> Scala<br>  <span class="token atrule key">sparkVersion</span><span class="token punctuation">:</span> 3.5.1<br>  <span class="token atrule key">mode</span><span class="token punctuation">:</span> cluster<br>  <span class="token atrule key">image</span><span class="token punctuation">:</span> hello<span class="token punctuation">-</span>spark<span class="token punctuation">:</span>latest<br>  <span class="token atrule key">mainClass</span><span class="token punctuation">:</span> Main<br>  <span class="token atrule key">mainApplicationFile</span><span class="token punctuation">:</span> local<span class="token punctuation">:</span>///app/work/application.jar<br>  <span class="token atrule key">deps</span><span class="token punctuation">:</span><br>    <span class="token atrule key">packages</span><span class="token punctuation">:</span><br>      <span class="token punctuation">-</span> org.apache.spark<span class="token punctuation">:</span>spark<span class="token punctuation">-</span>sql<span class="token punctuation">-</span>kafka<span class="token punctuation">-</span>0<span class="token punctuation">-</span>10_2.12<span class="token punctuation">:</span>3.5.1<br>  <span class="token atrule key">sparkConf</span><span class="token punctuation">:</span><br>    <span class="token atrule key">"spark.driver.extraJavaOptions"</span><span class="token punctuation">:</span> <span class="token string">"-Divy.cache.dir=/tmp -Divy.home=/tmp"</span><br>  <span class="token atrule key">driver</span><span class="token punctuation">:</span><br>    <span class="token atrule key">memory</span><span class="token punctuation">:</span> 512m<br>    <span class="token atrule key">labels</span><span class="token punctuation">:</span><br>      <span class="token atrule key">version</span><span class="token punctuation">:</span> 3.5.1<br>    <span class="token atrule key">serviceAccount</span><span class="token punctuation">:</span> sparksubmit<br>    <span class="token atrule key">env</span><span class="token punctuation">:</span><br>      <span class="token punctuation">-</span> <span class="token atrule key">name</span><span class="token punctuation">:</span> KAFKA_USER<br>        <span class="token atrule key">value</span><span class="token punctuation">:</span> <span class="token string">"user1"</span><br>      <span class="token punctuation">-</span> <span class="token atrule key">name</span><span class="token punctuation">:</span> KAFKA_PASS<br>        <span class="token atrule key">valueFrom</span><span class="token punctuation">:</span><br>          <span class="token atrule key">secretKeyRef</span><span class="token punctuation">:</span><br>            <span class="token atrule key">name</span><span class="token punctuation">:</span> kafka<span class="token punctuation">-</span>user<span class="token punctuation">-</span>passwords<br>            <span class="token atrule key">key</span><span class="token punctuation">:</span> client<span class="token punctuation">-</span>passwords<br>  <span class="token atrule key">executor</span><span class="token punctuation">:</span><br>    <span class="token atrule key">memory</span><span class="token punctuation">:</span> 512m<br>    <span class="token atrule key">instances</span><span class="token punctuation">:</span> <span class="token number">3</span><br>    <span class="token atrule key">labels</span><span class="token punctuation">:</span><br>      <span class="token atrule key">version</span><span class="token punctuation">:</span> 3.5.1</code></pre><p>Note that the secret <code>kafka-user-passwords</code> is the secret that created by <code>Binami</code> Kafka helm chart above. If you use another type of chart please double check the name or you can also create a secret yourself and update the deployment file accordingly.</p><share-widget><button aria-label=Share href=https://thefortunedays.com/articles/getting-started-with-apache-spark/ on-click=share><div></div></button></share-widget><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Getting Started With Apache Spark","image":["https://thefortunedays.com/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.jpg"],"author":"Thanh","genre":"Blog","publisher":{"@type":"Organization","name":"Thanh","logo":{"@type":"ImageObject","url":"/img/favicon/favicon-192x192.png?hash=735479e2c4"}},"url":"https://thefortunedays.com/articles/getting-started-with-apache-spark/","mainEntityOfPage":"https://thefortunedays.com/articles/getting-started-with-apache-spark/","datePublished":"2024-03-10","dateModified":"2024-04-17","description":"This blog post is a summary of my presentation on Apache Spark Overview with a basic Kafka streaming application. It includes some basic..."}</script></article></main><footer><a href=/about/ >Thanh</a></footer>