<!DOCTYPE html><html domain=thefortunedays.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href="/img/favicon/favicon-192x192.png?hash=735479e2c4" rel=icon type=image/png><meta content=#ff577d name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>Getting Started With Apache Spark</title><meta content="Getting Started With Apache Spark" property=og:title><meta content="Getting started with Apache Spark and Structure Streaming on Kubernetes." name=description><meta content="Getting started with Apache Spark and Structure Streaming on Kubernetes." property=og:description><meta content=summary_large_image name=twitter:card><meta content=@phamthethanh108 name=twitter:site><meta content=@phamthethanh108 name=twitter:creator><meta content=https://thefortunedays.com/img/remote/Z1o6V9l.jpg property=og:image><link href=https://thefortunedays.com/articles/getting-started-with-apache-spark/ rel=canonical><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="The Fortune Days"><link href=/ rel=preconnect crossorigin=""><link href=/fonts/CyberwayRiders.woff2 rel=preload type=font/woff2 as=font crossorigin=""><link href=/fonts/Inter-3.19.var.woff2 rel=preload type=font/woff2 as=font crossorigin=""><script async defer src="/js/min.js?hash=8db32dcb3a"></script><script async defer src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4025327352745846" crossorigin=anonymous></script><script csp-hash="">if (/Mac OS X/.test(navigator.userAgent))document.documentElement.classList.add('apple')</script><style>:root{--primary: #ff577d;--primary-dark:#ff184c;--text-color: #F5F5F7;--bg-color: 	#003062;--nav-bg-color: #ff577d;--nav-text-color: #fff;--footer-bg-color: #091833;--footer-text-color: #999;--main-width: calc(100vw - 3em)}main img{content-visibility:auto}article *{scroll-margin-top:50px}header nav{position:fixed;padding:.575em 1.5em;background:var(--nav-bg-color);color:var(--nav-text-color);font-weight:200;text-align:right}@media (min-width:37.5em){:root{--main-width: calc(37.5em - 3em)}}dialog,share-widget{position:fixed;opacity:.9}share-widget{right:20px;bottom:20px}share-widget div{width:30px;height:30px;background-image:url(/img/share.svg);background-repeat:no-repeat;background-position:center}.apple share-widget div{background-image:url(/img/share-apple.svg)}body,share-widget button{margin:0}share-widget button:active{transform:scale(1.2)}dialog{background-color:#0a9cf5;z-index:1000}#nav{z-index:2;position:relative}#reading-progress,header nav{z-index:1;width:100vw;left:0;top:0}#reading-progress{background-color:var(--primary);position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}@font-face{font-display:optional;font-family:"Inter UI";font-weight:100 900;font-style:oblique 0deg 10deg;src:url(/fonts/Inter-3.19.var.woff2) format("woff2")}@font-face{font-display:optional;font-family:"CyberwayRiders";font-weight:100 900;font-style:oblique 0deg 10deg;src:url(/fonts/CyberwayRiders.woff2) format("woff2")}button,html{line-height:1.15}html{-webkit-text-size-adjust:100%;font-family:"Inter UI",sans-serif;--font-family: "Inter UI", sans-serif}h1{font-size:3em;line-height:1.25;margin:.67em 0 .5em;font-size:2.074rem}a{background-color:transparent;color:#f9c412;text-decoration:none;color:var(--primary)}b{font-weight:700}img{border-style:none;max-width:100%;height:auto;margin:0 auto}button{font-family:inherit;font-size:100%;overflow:visible;text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}summary{display:list-item}h2{font-size:2.5em;line-height:1.2;margin-bottom:.6em}h1,h2,h3{margin-bottom:1.36rem}h3{font-size:2em;line-height:1.125;margin-bottom:.75em;font-size:1.4rem;line-height:1.6rem}body,p,pre,ul{font-size:1em}p,pre,ul{margin-bottom:1.5em}h1,h2{line-height:2.4rem}h2{font-size:1.728rem}body,p,pre,ul{font-size:1rem;line-height:1.6}p,pre,ul{margin-bottom:1.36rem}@media (min-width:600px){h1{font-size:4.3978rem;line-height:4.4rem}h2{font-size:3.1097rem;line-height:3.52rem}h3{font-size:2.1989rem;line-height:2.64rem}body,p,pre,ul{font-size:1.1rem;line-height:1.6}h1,h2,h3,p,pre,ul{margin-bottom:1.496rem}}code,pre{overflow-x:auto}pre{font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace}pre:has(code:not([class])){background:#2d2d2d}pre code:not([class]){color:#ccc;padding:0;overflow-x:scroll}button,code{border-radius:.3em}code{color:#e2777a;padding:0 .3em;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:90%;background:#2d2d2d}h1,h2,h3{font-family:var(--font-family)}h2{font-style:italic}a:hover{text-decoration:underline}button{max-width:100%;background:#f2f2f2;color:#191919;cursor:pointer;display:inline-block;padding:.75em 1.5em;text-align:center;margin:0 .75em 1.5em 0}button:hover{background:#d9d9d9;color:#000}button:not([disabled]),button[type=submit]{background:#f9c412;color:#181818}button:not([disabled]):hover,button[type=submit]:hover{background:#ba9005;color:#000}*{border:0;box-sizing:border-box}body,header nav a{font-family:var(--font-family)}body{background:var(--bg-color);color:var(--text-color)}header{padding:4em 1.5em 3em;width:37.5em;margin:0 auto;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header p,ul{margin-top:0}header nav h1{float:left;font-size:inherit;line-height:inherit;margin:0;text-align:left}header nav a{font-weight:700;text-decoration:none;color:var(--nav-text-color);margin-left:1.5em;font-family:'CyberwayRiders'}header nav a:first-of-type{margin-left:auto}header nav a:last-of-type{margin-right:1.5em}main{max-width:70rem;margin:0 auto}footer{background:var(--footer-bg-color);color:var(--footer-text-color);padding:3em;text-align:center}footer>*{margin:1.5em;font-family:'CyberwayRiders'}footer nav a img{vertical-align:middle}footer nav,footer p{font-size:90%}article{max-width:100%;padding:1.5em;width:37.5em;margin:0 auto}li ul{margin-bottom:0}code[class*=language-],pre[class*=language-]{color:#ccc;background:0 0;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-]{padding:0}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background:#2d2d2d}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.token.comment{color:#999}.token.punctuation{color:#ccc}.token.namespace{color:#e2777a}.token.function{color:#f08d49}.token.class-name,.token.property{color:#f8c555}.token.builtin,.token.keyword{color:#cc99cd}.token.string,.token.variable{color:#7ec699}.token.operator,.token.url{color:#67cdcc}.token.inserted{color:green}button:not([disabled]),button[type=submit]{background:var(--primary)}button:not([disabled]):hover,button[type=submit]:hover{background:var(--primary-dark)}.about,.tags{min-height:80vh}.about h1,.home h1{font-family:'CyberwayRiders';padding-bottom:1em}.tags h1,article aside,article h1{text-align:center}article aside{font-style:italic;padding-bottom:4.25em}.tags h1{font-family:'CyberwayRiders';padding-bottom:1em}@media (min-width:600px){.about,.tags{min-height:calc(100vh - 360px)}}</style><header><nav><div id=nav><h1><a href=/ title=Homepage>The Fortune Days</a></h1><a href=/tags/ >Tags</a> <a href=/about/ >About</a></div><div id=reading-progress aria-hidden=true></div></nav><dialog id=message></dialog></header><main><article><h1>Getting Started With Apache Spark</h1><aside>6 min read.</aside><p>This blog post is a summary of my presentation on Apache Spark Overview with a basic Kafka streaming application. It includes some basic steps to create and deploy a simple Spark application. This is not a comprehensive tutorial, so for more information, please visit the Apache Spark <a href=https://spark.apache.org/docs/latest/ >website</a>.<h2 id=prerequisite>Prerequisite <a href=#prerequisite class=direct-link>#</a></h2><ul><li><a href=https://www.docker.com/ >Docker</a><li><a href=https://minikube.sigs.k8s.io/docs/ >Minikube</a></ul><h2 id=apache-spark-overview>Apache Spark Overview <a href=#apache-spark-overview class=direct-link>#</a></h2><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.avif 1920w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1280w.avif 1280w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-640w.avif 640w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-320w.avif 320w" type=image/avif><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.webp 1920w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1280w.webp 1280w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-640w.webp 640w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-320w.webp 320w" type=image/webp><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.jpg 1920w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1280w.jpg 1280w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-640w.jpg 640w, /img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-320w.jpg 320w" type=image/jpeg><img alt="Introduction to Apache Spark" decoding=async height=3186 loading=lazy src=/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.jpg style="background-size:cover;background-image:url(&#34;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 893 3186'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAPCAIAAABMVPnqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAjElEQVQImSWOyw7DMAjA+P8v3HnPVF1pgEAIVcsU7WzLMmRmJW6imQnWu/XRPa5McGuttWP4UAVrRMyVaLjDtm211qWUiACh2kTkT0R1R2TmWfPh5n67ly8pxHGQ+fJ4ju6QmR7BLFM7IsyM6t67gakQMa5vxgKIyNJwfdH2ATWdBzuaClx5qs/bvM4fnKmr3FK+eDQAAAAASUVORK5CYII='%3E%3C/image%3E%3C/svg%3E&#34;)" width=893></picture><h2 id=build-spark-application>Build Spark Application <a href=#build-spark-application class=direct-link>#</a></h2><p><code>Main.scala</code><pre class=language-scala><code class=language-scala><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span></span>SparkSession<br><br><br><br><span class="token keyword">object</span> Main <span class="token punctuation">{</span><br>  <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span><br>    <span class="token keyword">val</span> spark<span class="token operator">:</span>SparkSession <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">"hello-spark"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">import</span> <span class="token namespace">spark<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span></span>_<br>    <span class="token keyword">val</span> kafkaOpts <span class="token operator">=</span> Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">(</span><br>      <span class="token string">"kafka.bootstrap.servers"</span><span class="token operator">-></span> <span class="token string">"kafka:9092"</span><span class="token punctuation">,</span><br>      <span class="token string">"subscribe"</span><span class="token operator">-></span> <span class="token string">"test"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.sasl.mechanism"</span><span class="token operator">-></span> <span class="token string">"PLAIN"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.security.protocol"</span> <span class="token operator">-></span> <span class="token string">"SASL_PLAINTEXT"</span><span class="token punctuation">,</span><br>      <span class="token string">"kafka.sasl.jaas.config"</span><span class="token operator">-></span> <span class="token string triple-quoted-string">"""org.apache.kafka.common.security.plain.PlainLoginModule required username="user1" password="kk3gaqZRly";"""</span><span class="token punctuation">,</span><br>    <span class="token punctuation">)</span><br>    <span class="token keyword">val</span> df <span class="token operator">=</span> spark<br>      <span class="token punctuation">.</span>readStream<br>      <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"kafka"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>options<span class="token punctuation">(</span>kafkaOpts<span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">val</span> ds <span class="token operator">=</span> df<span class="token punctuation">.</span>selectExpr<span class="token punctuation">(</span><span class="token string">"CAST(key AS STRING)"</span><span class="token punctuation">,</span> <span class="token string">"CAST(value AS STRING)"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>as<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token punctuation">]</span><br><br>    ds<span class="token punctuation">.</span>writeStream<br>      <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"console"</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>outputMode<span class="token punctuation">(</span><span class="token string">"append"</span><span class="token punctuation">)</span><br>      <span class="token comment">//.trigger(Trigger.Continuous(1))</span><br>      <span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><br>      <span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span><br>  <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre><p>Note that in the above code we put the user/password of Kafka directly in the code which is just for simplicity & testing purpose. For production please store them in Vault or Secret instead.<p><code>build.sbt</code><pre class=language-shell><code class=language-shell>ThisBuild / version :<span class="token operator">=</span> <span class="token string">"0.1.0-SNAPSHOT"</span><br><br>ThisBuild / scalaVersion :<span class="token operator">=</span> <span class="token string">"2.12.18"</span><br><br>libraryDependencies <span class="token operator">+=</span> <span class="token string">"org.apache.spark"</span> %% <span class="token string">"spark-core"</span> % <span class="token string">"3.5.1"</span><br>libraryDependencies <span class="token operator">+=</span> <span class="token string">"org.apache.spark"</span> %% <span class="token string">"spark-sql"</span> % <span class="token string">"3.5.1"</span><br>libraryDependencies <span class="token operator">+=</span> <span class="token string">"org.apache.spark"</span> %% <span class="token string">"spark-sql-kafka-0-10"</span> % <span class="token string">"3.5.1"</span> % Test<br><br>Compile / run / mainClass :<span class="token operator">=</span> Some<span class="token punctuation">(</span><span class="token string">"Main"</span><span class="token punctuation">)</span><br><br>lazy val root <span class="token operator">=</span> <span class="token punctuation">(</span>project <span class="token keyword">in</span> file<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">))</span><br>  .settings<span class="token punctuation">(</span><br>    name :<span class="token operator">=</span> <span class="token string">"hello-spark"</span><br>  <span class="token punctuation">)</span><br></code></pre><p><code>Dockerfile</code><pre class=language-dockerfile><code class=language-dockerfile><span class="token instruction"><span class="token keyword">ARG</span> SCALA_VERSION=2.12</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_JAR_NAME=hello-spark_2.12-0.1.0-SNAPSHOT.jar</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_MAIN_CLASS=Main</span><br><br><span class="token instruction"><span class="token keyword">FROM</span> sbtscala/scala-sbt:graalvm-ce-22.3.3-b1-java17_1.9.9_2.12.18 <span class="token keyword">as</span> build</span><br><span class="token instruction"><span class="token keyword">WORKDIR</span> /app</span><br><span class="token instruction"><span class="token keyword">COPY</span> . .</span><br><span class="token instruction"><span class="token keyword">RUN</span> sbt package</span><br><br><span class="token instruction"><span class="token keyword">FROM</span> apache/spark:3.5.1-scala2.12-java17-ubuntu</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_JAR_NAME</span><br><span class="token instruction"><span class="token keyword">ARG</span> ARG_MAIN_CLASS</span><br><span class="token instruction"><span class="token keyword">ARG</span> SCALA_VERSION</span><br><span class="token instruction"><span class="token keyword">COPY</span> <span class="token options"><span class="token property">--from</span><span class="token punctuation">=</span><span class="token string">build</span></span> /app/target/scala-<span class="token variable">${SCALA_VERSION}</span>/<span class="token variable">${ARG_JAR_NAME}</span> /app/work/application.jar</span></code></pre><p>Build application Docker image<pre><code>eval $(minikube -p minikube docker-env)
docker build -t hello-spark:latest .
</code></pre><h2 id=deploy-kafka>Deploy Kafka <a href=#deploy-kafka class=direct-link>#</a></h2><pre class=language-shell><code class=language-shell>helm <span class="token function">install</span> kafka oci://registry-1.docker.io/bitnamicharts/kafka</code></pre><p>Notice the notes after you installed Kafka, it includes information for authenticate with Kafka cluster. Otherwise you can check the <code>server.properties</code>:<pre class=language-shell><code class=language-shell>kubectl <span class="token builtin class-name">exec</span> kafka-controller-0 -- <span class="token function">cat</span> /opt/bitnami/kafka/config/server.properties</code></pre><p>Let run Kafka-client to create topic and test our Kafka cluster.<pre class=language-shell><code class=language-shell>kubectl run kafka-client --rm -ti --image bitnami/kafka:3.6.1-debian-12-r12 -- <span class="token function">bash</span><br><span class="token builtin class-name">cd</span> /opt/bitnami/kafka/bin<br><br><span class="token function">cat</span> <span class="token operator">>></span> client.conf <span class="token operator">&lt;&lt;</span><span class="token string">EOF<br>security.protocol=SASL_PLAINTEXT<br>sasl.mechanism=PLAIN<br>sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="user1" password="kk3gaqZRly";<br>EOF</span><br><br>kafka-topics.sh --create --topic <span class="token builtin class-name">test</span> --bootstrap-server kafka:9092 --command-config client.conf<br><br>kafka-console-producer.sh --topic <span class="token builtin class-name">test</span> --request-required-acks all --bootstrap-server kafka:9092 --producer.config client.conf<br><br>kafka-console-consumer.sh --topic <span class="token builtin class-name">test</span> --bootstrap-server kafka:9092 --consumer.config client.conf<br></code></pre><h2 id=deployment>Deployment <a href=#deployment class=direct-link>#</a></h2><p>Deploy Spark application to Kubernetes cluster require a ServiceAccount, hence we need to create it first:<pre class=language-shell><code class=language-shell>kubectl create serviceaccount sparksubmit<br>kubectl create clusterrolebinding sparksubmit-role --clusterrole<span class="token operator">=</span>edit --serviceaccount<span class="token operator">=</span>default:sparksubmit --namespace<span class="token operator">=</span>default</code></pre><p>Deploy a Spark application to Kubernetes can be done by 2 ways:<ul><li>Using spark-submit<li>Using Spark-operator</ul><h3 id=deploy-using-spark-submit>Deploy using spark-submit <a href=#deploy-using-spark-submit class=direct-link>#</a></h3><p>In this deployment mode, we submit the application using <code>spark-submit</code> from outside of the Kubernetes cluster. To install the spark-submit, you can download and setup the Apache Spark on your machine or pipeline:<pre><code>wget https://www.apache.org/dyn/closer.lua/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
tar -xzf spark-3.5.1-bin-hadoop3.tgz -C ~/bin

export PATH=$PATH:~/bin/spark-3.5.1-bin-hadoop3
export PATH=$PATH:~/bin/spark-3.5.1-bin-hadoop3/bin
export SPARK_HOME=~/bin/spark-3.5.1-bin-hadoop3
export HADOOP_HOME=~/bin/spark-3.5.1-bin-hadoop3
</code></pre><p>Note that for testing the Spark application locally, you also must install Java (17) and Scala (2.12) on your local machine.<p>After installed <code>spark-submit</code>, go ahead to submit the Spark application:<pre><code>spark-submit \
--master k8s://https://127.0.0.1:32769 \
--deploy-mode cluster \
--name hello-spark \
--class Main \
--conf spark.kubernetes.namespace=default \
--conf spark.kubernetes.container.image=hello-spark:latest \
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=sparksubmit \
--conf spark.kubernetes.authenticate.submission.caCertFile=~/.minikube/ca.crt \
local:///app/work/application.jar
</code></pre><p>Notes:<ul><li>Run <code>kubectl cluster-info</code> to get the URL of your K8S cluster.<li>Replace the <code>caCertFile</code> to your certificate file. If relative path doesn't work, change to absolute path.</ul><p>After deployed, you would see the driver and executors got deployed in your K8S:<pre><code>kubectl get pod
NAME                                  READY   STATUS    RESTARTS      AGE
hello-spark-3605288e2b93f981-exec-1   1/1     Running   0             10s
hello-spark-3605288e2b93f981-exec-2   1/1     Running   0             10s
hello-spark-6da76b8e2b92dd56-driver   1/1     Running   0             82s
kafka-controller-0                    1/1     Running   1 (12m ago)   14h
kafka-controller-1                    1/1     Running   1 (12m ago)   14h
kafka-controller-2                    1/1     Running   1 (12m ago)   14h
</code></pre><p>Once the driver and the executors are up and running, you can start the Spark UI to see the application detail as well as start sending some events to Kafka and check for the output from log of the driver.<pre><code>kubectl port-forward hello-spark-6da76b8e2b92dd56-driver 4040:4040
</code></pre><p>If you would like to kill the application, just run spark-submit with <code>-kill</code> option:<pre><code>spark-submit --kill hello-spark-d5358c8e1cd2e8b0-driver --master k8s://https://127.0.0.1:32769
</code></pre><h3 id=deploy-using-spark-operator>Deploy using Spark-Operator <a href=#deploy-using-spark-operator class=direct-link>#</a></h3><p>Deploy Spark application using Spark operator is recommended for production. In this deployment model, we will install <code>spark-operator</code> inside K8S cluster, spark-operator will listen on any application deployed to the K8S with application kind as <code>SparkApplication</code> and will help us to submit Spark application to the K8S for us.<p>You can read more detail on using spark-operator at their <a href=https://googlecloudplatform.github.io/spark-on-k8s-operator/docs/user-guide.html>user-guide</a><p>Install spark-operator:<pre class=language-shell><code class=language-shell>helm repo <span class="token function">add</span> spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator<br>helm <span class="token function">install</span> spark-operator spark-operator/spark-operator --namespace spark-operator --create-namespace --set <span class="token variable assign-left">enableWebhook</span><span class="token operator">=</span>true</code></pre><p>Note that the option <code>--set enableWebhook=true</code> must be enabled if you would like to use some certain Spark operator features such as setting <code>env</code> or GPU for driver and executor. See more <a href=https://googlecloudplatform.github.io/spark-on-k8s-operator/docs/quick-start-guide.html#about-the-mutating-admission-webhook>here</a><p>Check for the deployment status:<pre><code>kubectl get pod -n spark-operator
NAME                              READY   STATUS    RESTARTS      AGE
spark-operator-675d97df85-c2b9g   1/1     Running   2 (46m ago)   14h
</code></pre><p>Create application deployment file <code>spark-application.yaml</code><pre><code>apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: hello-spark
  namespace: default
spec:
  type: Scala
  sparkVersion: 3.5.1
  mode: cluster
  image: hello-spark:latest
  mainClass: Main
  mainApplicationFile: local:///app/work/application.jar
  deps:
    packages:
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
  sparkConf:
    "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp"
  driver:
    memory: 512m
    labels:
      version: 3.5.1
    serviceAccount: sparksubmit
  executor:
    memory: 512m
    instances: 3
    labels:
      version: 3.5.1
</code></pre><p>Deploy it to the cluster<pre class=language-shell><code class=language-shell>kubectl apply -f spark-application.yaml</code></pre><p>Wait for sometime for spark-operator to detect and submit the application and then you can start testing the application:<pre><code>kubectl get pod
NAME                                  READY   STATUS    RESTARTS      AGE
hello-spark-driver                    1/1     Running   0             54s
hello-spark-efd4f18e2bb34946-exec-1   1/1     Running   0             4s
hello-spark-efd4f18e2bb34946-exec-2   1/1     Running   0             4s
hello-spark-efd4f18e2bb34946-exec-3   1/1     Running   0             4s
kafka-controller-0                    1/1     Running   1 (46m ago)   14h
kafka-controller-1                    1/1     Running   1 (46m ago)   14h
kafka-controller-2                    1/1     Running   1 (46m ago)   14h
</code></pre><share-widget><button aria-label=Share href=https://thefortunedays.com/articles/getting-started-with-apache-spark/ on-click=share><div></div></button></share-widget><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Getting Started With Apache Spark","image":["https://thefortunedays.com/img/remote/articles/getting-started-with-apache-spark/apache-spark-overview-1920w.jpg"],"author":"Thanh Pham","genre":"Blog","publisher":{"@type":"Organization","name":"Thanh Pham","logo":{"@type":"ImageObject","url":"/img/favicon/favicon-192x192.png?hash=735479e2c4"}},"url":"https://thefortunedays.com/articles/getting-started-with-apache-spark/","mainEntityOfPage":"https://thefortunedays.com/articles/getting-started-with-apache-spark/","datePublished":"2024-03-10","dateModified":"2024-03-13","description":"This blog post is a summary of my presentation on Apache Spark Overview with a basic Kafka streaming application. It includes some basic..."}</script></article></main><footer><a href=/about/ >Thanh Pham</a></footer>