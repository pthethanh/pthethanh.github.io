<!DOCTYPE html><html domain=thefortunedays.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href=/favicon.svg rel=icon type=image/svg+xml><meta content=#ff577d name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>Apache Spark Structured Streaming</title><meta content="Apache Spark Structured Streaming" property=og:title><meta content="Getting started with Apache Spark and Structure Streaming on Kubernetes." name=description><meta content="Getting started with Apache Spark and Structure Streaming on Kubernetes." property=og:description><meta content=summary_large_image name=twitter:card><meta content=@phamthethanh108 name=twitter:site><meta content=@phamthethanh108 name=twitter:creator><meta content=https://thefortunedays.com/img/remote/Z1o6V9l.jpg property=og:image><link href=https://thefortunedays.com/articles/getting-started-with-apache-spark-structured-streaming/ rel=canonical><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="The Fortune Days"><link href=/ rel=preconnect crossorigin=""><link href=/fonts/CyberwayRiders.woff2 rel=preload type=font/woff2 as=font crossorigin=""><link href=/fonts/Inter-3.19.var.woff2 rel=preload type=font/woff2 as=font crossorigin=""><script async defer src="/js/min.js?hash=8db32dcb3a"></script><script async defer src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4025327352745846" crossorigin=anonymous></script><script csp-hash="">if (/Mac OS X/.test(navigator.userAgent))document.documentElement.classList.add('apple')</script><style>:root{--primary: #ff577d;--primary-dark:#ff184c;--text-color: #F5F5F7;--bg-color: 	#003062;--nav-bg-color: #ff577d;--nav-text-color: #fff;--footer-bg-color: #091833;--footer-text-color: #999;--main-width: calc(100vw - 3em)}main img{content-visibility:auto}article *{scroll-margin-top:50px}header nav{position:fixed;padding:.575em 1.5em;background:var(--nav-bg-color);color:var(--nav-text-color);font-weight:200;text-align:right}@media (min-width:37.5em){:root{--main-width: calc(37.5em - 3em)}}dialog,share-widget{position:fixed;opacity:.9}share-widget{right:20px;bottom:20px}share-widget div{width:30px;height:30px;background-image:url(/img/share.svg);background-repeat:no-repeat;background-position:center}.apple share-widget div{background-image:url(/img/share-apple.svg)}body,share-widget button{margin:0}share-widget button:active{transform:scale(1.2)}dialog{background-color:#0a9cf5;z-index:1000}#nav{z-index:2;position:relative}#reading-progress,header nav{z-index:1;width:100vw;left:0;top:0}#reading-progress{background-color:var(--primary);position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}@font-face{font-display:optional;font-family:"Inter UI";font-weight:100 900;font-style:oblique 0deg 10deg;src:url(/fonts/Inter-3.19.var.woff2) format("woff2")}@font-face{font-display:optional;font-family:"CyberwayRiders";font-weight:100 900;font-style:oblique 0deg 10deg;src:url(/fonts/CyberwayRiders.woff2) format("woff2")}html{line-height:1.15;-webkit-text-size-adjust:100%;font-family:"Inter UI",sans-serif;--font-family: "Inter UI", sans-serif}h1{font-size:3em;line-height:1.25;margin:.67em 0 .5em;font-size:2.074rem}a{background-color:transparent;color:#f9c412;text-decoration:none;color:var(--primary)}img{border-style:none;max-width:100%;height:auto;margin:0 auto}button,select{font-family:inherit;font-size:100%;line-height:1.15;text-transform:none}button{overflow:visible}select{margin:0}[type=button],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}h2{font-size:2.5em;line-height:1.2;margin-bottom:.6em}body,p,pre,ul{font-size:1em}p,pre,ul{margin-bottom:1.5em}h1,h2{line-height:2.4rem;margin-bottom:1.36rem}h2{font-size:1.728rem}body,p,pre,ul{font-size:1rem;line-height:1.6}p,pre,ul{margin-bottom:1.36rem}@media (min-width:600px){h1{font-size:4.3978rem;line-height:4.4rem}h2{font-size:3.1097rem;line-height:3.52rem}body,p,pre,ul{font-size:1.1rem;line-height:1.6}h1,h2,p,pre,ul{margin-bottom:1.496rem}}code,pre{overflow-x:auto}pre{font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace}pre:has(code:not([class])){background:#2d2d2d}pre code:not([class]){color:#ccc;padding:0;overflow-x:scroll}code{border-radius:.3em;color:#e2777a;padding:0 .3em;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:90%;background:#2d2d2d}h1,h2{font-family:var(--font-family)}h2{font-style:italic}a:hover{text-decoration:underline}dt{font-weight:600;padding-top:.75em;padding-left:.75em}button,select{border-radius:.3em;max-width:100%}select{margin-bottom:1.5em;display:inline;border:1px solid #595959;padding:.75em}button{background:#f2f2f2;color:#191919;cursor:pointer;display:inline-block;padding:.75em 1.5em;text-align:center;margin:0 .75em 1.5em 0}button:hover{background:#d9d9d9;color:#000}button:not([disabled]){background:#f9c412;color:#181818;background:var(--primary)}button:not([disabled]):hover{background:#ba9005;color:#000;background:var(--primary-dark)}*{border:0;box-sizing:border-box}body,header nav a{font-family:var(--font-family)}body{background:var(--bg-color);color:var(--text-color)}header{padding:4em 1.5em 3em;width:37.5em;margin:0 auto;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header p,ul{margin-top:0}header nav h1{float:left;font-size:inherit;line-height:inherit;margin:0;text-align:left}header nav a{font-weight:700;text-decoration:none;color:var(--nav-text-color);margin-left:1.5em;font-family:'CyberwayRiders'}header nav a:first-of-type{margin-left:auto}header nav a:last-of-type{margin-right:1.5em}main{max-width:70rem;margin:0 auto}footer{background:var(--footer-bg-color);color:var(--footer-text-color);padding:3em;text-align:center}footer>*{margin:1.5em;font-family:'CyberwayRiders'}footer nav a img{vertical-align:middle}footer nav,footer p{font-size:90%}article{max-width:100%;padding:1.5em;width:37.5em;margin:0 auto}li ul{margin-bottom:0}.about,.tags{min-height:80vh}.about h1,.home h1{font-family:'CyberwayRiders';padding-bottom:1em}.home h1{color:#fff;text-shadow:0 0 7px #fff,0 0 42px #0a9cf5}.tags h1,article aside,article h1{text-align:center}article aside{font-style:italic;padding-bottom:4.25em}.tags h1{font-family:'CyberwayRiders';padding-bottom:1em}@media (min-width:600px){.about,.tags{min-height:calc(100vh - 360px)}}</style><header><nav><div id=nav><h1><a href=/ title=Homepage>The Fortune Days</a></h1><a href=/tags/ >Tags</a> <a href=/about/ >About</a></div><div id=reading-progress aria-hidden=true></div></nav><dialog id=message></dialog></header><main><article><h1>Apache Spark Structured Streaming</h1><aside>5 min read.</aside><p>In this blog post, we're going to explore features of Apache Spark Structured Streaming by building an application that read structured data from Kafka and then streaming it into PostgresSQL.<p>This blog post is the next session of the previous <a href=/articles/getting-started-with-apache-spark/ >Getting Started With Apache Spark</a>, hence please visit that post for basic setup as we will not repeat them here.<p>Note that we will use <code>spark-operator</code> to deploy our Spark application, hence please make sure you installed the spark-operator accordingly. If you have not done it yet, take a look at it <a href=https://thefortunedays.com/articles/getting-started-with-apache-spark/#deploy-using-spark-operator>here</a><p>Please notice that this blog post's purpose is to provide steps for those who want to try with Spark streaming, it's not a comprehensive tutorial. If you expect a comprehensive tutorial please visit Apache Spark guideline.<h2 id=prerequisite>Prerequisite <a href=#prerequisite class=direct-link>#</a></h2><ul><li><a href=https://www.docker.com/ >Docker</a><li><a href=https://minikube.sigs.k8s.io/docs/ >Minikube</a><li><a href=https://thefortunedays.com/articles/getting-started-with-apache-spark/#deploy-kafka>Kafka</a>: with topic <code>test</code> created in advance.<li><a href=https://thefortunedays.com/articles/getting-started-with-apache-spark/#deploy-using-spark-operator>spark-operator</a></ul><h2 id=install-postgres>Install PostGres <a href=#install-postgres class=direct-link>#</a></h2><p>Install PostGres using bitnami helm chart:<pre><code>helm install postgresql oci://registry-1.docker.io/bitnamicharts/postgresql
</code></pre><p>Connect to PostGres<pre><code>export POSTGRES_PASSWORD=$(kubectl get secret --namespace default postgresql -o jsonpath="{.data.postgres-password}" | base64 -d)

kubectl run postgresql-client --rm --tty -i --restart='Never' --namespace default --image docker.io/bitnami/postgresql:16.2.0-debian-12-r6 --env="PGPASSWORD=$POSTGRES_PASSWORD" \
      --command -- psql --host postgresql -U postgres -d postgres -p 5432;
</code></pre><p>Create database and user:<pre><code>CREATE USER spark WITH ENCRYPTED PASSWORD 'spark123';

CREATE DATABASE spark WITH OWNER spark;

\connect spark;
\dt
</code></pre><p>Create secret to store user/pass:<pre><code>kubectl create secret generic db-user-pass \
    --from-literal=username=spark \
    --from-literal=password='spark123'
</code></pre><h2 id=build-the-application>Build the application <a href=#build-the-application class=direct-link>#</a></h2><p><code>Main.scala</code><pre><code>import org.apache.spark.sql.functions.{col, from_json}
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{Dataset, Encoders, SaveMode, SparkSession}

import java.sql.Timestamp



object Main {
  def main(args: Array[String]): Unit = {
    val spark:SparkSession = SparkSession.builder()
      .appName("hello-spark-streaming")
      .getOrCreate()

    val kafkaUser = sys.env.getOrElse("KAFKA_USER", "user1")
    val kafkaPass = sys.env.getOrElse("KAFKA_PASS", "")
    val kafkaOpts = Map[String, String](
      "kafka.bootstrap.servers"-> "kafka:9092",
      "subscribe"-> "test",
      "kafka.sasl.mechanism"-> "PLAIN",
      "kafka.security.protocol" -> "SASL_PLAINTEXT",
      "kafka.sasl.jaas.config"-> s"""org.apache.kafka.common.security.plain.PlainLoginModule required username="$kafkaUser" password="$kafkaPass";""",
    )
    import spark.implicits._
    val df = spark
      .readStream
      .format("kafka")
      .options(kafkaOpts)
      .load()
      .selectExpr("CAST(value as STRING) as value")
      .select(from_json(col("value"),Event.schema).as[Event])

    val dbName = sys.env.getOrElse("POSTGRESQL_DBNAME","spark")
    val dbUser = sys.env.getOrElse("POSTGRESQL_USER", "spark")
    val dbPass = sys.env.getOrElse("POSTGRESQL_PASS", "")
    val postGreOpts = Map[String,String](
      "url" -> s"jdbc:postgresql://postgresql:5432/$dbName",
      "driver" -> "org.postgresql.Driver",
      "user" -> dbUser,
      "password" -> dbPass,
      "dbtable" -> "events",
    )
    df.writeStream
      .foreachBatch((batch :Dataset[Event], _: Long) => {
        batch.write
          .format("jdbc")
          .options(postGreOpts)
          .mode(SaveMode.Overwrite)
          .save()
      })
      .start()
      .awaitTermination()
  }
}

case class Event(uuid: String, typ: String, data: String, ts: Timestamp)

object Event {
  val schema: StructType = Encoders.product[Event].schema
}
</code></pre><p><code>build.sbt</code><pre><code>ThisBuild / version := "0.1.0-SNAPSHOT"

ThisBuild / scalaVersion := "2.12.18"

libraryDependencies += "org.apache.spark" %% "spark-core" % "3.5.1"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.5.1"
libraryDependencies += "org.apache.spark" %% "spark-sql-kafka-0-10" % "3.5.1" % Test
libraryDependencies += "org.postgresql" % "postgresql" % "42.7.2"

Compile / run / mainClass := Some("Main")

lazy val root = (project in file("."))
  .settings(
    name := "hello-spark"
  )

</code></pre><p><code>Dockerfile</code><pre><code>ARG SCALA_VERSION=2.12
ARG ARG_JAR_NAME=hello-spark_2.12-0.1.0-SNAPSHOT.jar
ARG ARG_MAIN_CLASS=Main

FROM sbtscala/scala-sbt:graalvm-ce-22.3.3-b1-java17_1.9.9_2.12.18 as build
WORKDIR /app
COPY . .
RUN sbt package

FROM apache/spark:3.5.1-scala2.12-java17-ubuntu
ARG ARG_JAR_NAME
ARG ARG_MAIN_CLASS
ARG SCALA_VERSION
COPY --from=build /app/target/scala-${SCALA_VERSION}/${ARG_JAR_NAME} /app/work/application.jar
</code></pre><p>Build Docker image:<pre><code>docker build -t hello-spark-streaming:latest .
</code></pre><p><code>spark-streaming-application.yaml</code><pre><code>apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: hello-spark-streaming
  namespace: default
spec:
  type: Scala
  sparkVersion: 3.5.1
  mode: cluster
  image: hello-spark-streaming:latest
  mainClass: Main
  mainApplicationFile: local:///app/work/application.jar
  deps:
    packages:
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
      - org.postgresql:postgresql:42.7.2
  sparkConf:
    "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp"
  driver:
    memory: 512m
    labels:
      version: 3.5.1
    serviceAccount: sparksubmit
    env:
      - name: KAFKA_USER
        value: user1
      - name: KAFKA_PASS
        valueFrom:
          secretKeyRef:
            name: kafka-user-passwords
            key: client-passwords
      - name: POSTGRESQL_DBNAME
        value: spark
      - name: POSTGRESQL_USER
        valueFrom:
          secretKeyRef:
            name: db-user-pass
            key: username
      - name: POSTGRESQL_PASS
        valueFrom:
          secretKeyRef:
            name: db-user-pass
            key: password
  executor:
    memory: 512m
    instances: 3
    labels:
      version: 3.5.1
</code></pre><p>Deploy the application:<pre><code>kubectl apply -f spark-streaming-application.yaml
</code></pre><p>Check status:<pre><code>kubectl get pod
NAME                                            READY   STATUS    RESTARTS       AGE
hello-spark-streaming-86b6a38e376d2a19-exec-1   1/1     Running   0              3m33s
hello-spark-streaming-86b6a38e376d2a19-exec-2   1/1     Running   0              3m33s
hello-spark-streaming-86b6a38e376d2a19-exec-3   1/1     Running   0              3m33s
hello-spark-streaming-driver                    1/1     Running   0              6m9s
kafka-client                                    1/1     Running   0              133m
kafka-controller-0                              1/1     Running   1 (161m ago)   6h15m
kafka-controller-1                              1/1     Running   1 (161m ago)   6h15m
kafka-controller-2                              1/1     Running   1 (161m ago)   6h15m
postgresql-0                                    1/1     Running   0              84m
postgresql-client                               1/1     Running   0              27m
</code></pre><p>Produce some messages:<pre><code>kafka-console-producer.sh --topic test --request-required-acks all --bootstrap-server kafka:9092 --producer.config client.conf

{"uuid":"f7a66f45-a671-4bf7-97af-cb58fffdbd54", "typ": "setup", "data":"event 1", "ts":"2024-03-11 09:09:45.847317+00"}
</code></pre><p>Check data in postgres:<pre><code>select * from events;
</code></pre><share-widget><button aria-label=Share href=https://thefortunedays.com/articles/getting-started-with-apache-spark-structured-streaming/ on-click=share><div></div></button></share-widget><script type=application/ld+json>{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Apache Spark Structured Streaming",
  "image": [],
  "author": "Thanh Pham", 
  "genre": "Blog", 
  "publisher": {
    "@type": "Organization",
    "name": "Thanh Pham",
    "logo": {
      "@type": "ImageObject",
      "url": "/img/favicon/favicon-192x192.png?hash=735479e2c4"
    }
  },
  "url": "https://thefortunedays.com/articles/getting-started-with-apache-spark-structured-streaming/",
  "mainEntityOfPage": "https://thefortunedays.com/articles/getting-started-with-apache-spark-structured-streaming/",
  "datePublished": "2024-03-11",
  "dateModified": "2024-03-13",
  "description": "In this blog post, we&#39;re going to explore features of Apache Spark Structured Streaming by building an application that read structured data..."
}</script></article></main><footer><a href=/about/ >Thanh Pham</a></footer>